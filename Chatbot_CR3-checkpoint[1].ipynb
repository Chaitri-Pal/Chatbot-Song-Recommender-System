{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import tensorflow\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "#from numpy import arange\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open(\"intents removed.json\").read())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "doc_x = []\n",
    "doc_y = []\n",
    "ignore = ['?','.',',','!',')','(',':',';','\\'']\n",
    "\n",
    "for intent in intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        word_tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_tokens)\n",
    "        doc_x.append(word_tokens)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "        \n",
    "print(\"words:\", words)\n",
    "print(\"\\n\\n\\n\\n\\n\\doc_x:\", doc_x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30983b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore]\n",
    "words = sorted(set(words))\n",
    "classes = sorted(classes)\n",
    "\n",
    "pickle.dump(words, open(\"word_CR3.pkl\",\"wb\"))\n",
    "pickle.dump(classes, open(\"classes_CR3.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b99667",
   "metadata": {},
   "outputs": [],
   "source": [
    "training =[]\n",
    "output = []\n",
    "output_empty = [0]*len(classes)\n",
    "\n",
    "for i, doc in enumerate(doc_x):\n",
    "    bag = []\n",
    "    doc_words = doc\n",
    "    doc_words = [lemmatizer.lemmatize(wrd.lower()) for wrd in doc_words]\n",
    "    #print(doc_words)\n",
    "    for w in words:\n",
    "        if w in doc_words:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    output_row = output_empty[:]\n",
    "    output_row[classes.index(doc_y[i])] = 1\n",
    "    \n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "#print(\"\\n training: \",training)\n",
    "#print(\"\\n\\n\\n\\n output: \", output)\n",
    "training = np.array(training)\n",
    "output = np.array(output)\n",
    "\n",
    "#print(words)\n",
    "#print(doc_words)\n",
    "print(training)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(training[0]),), activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output[0]), activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "hist_CR3 = model.fit(training, output, epochs = 200, batch_size = 5, verbose=1)\n",
    "model.save(\"Chatbot_CR3.h5\",hist_CR3)\n",
    "print(\"MODEL CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open(\"word_CR3.pkl\",\"rb\"))\n",
    "classes = pickle.load(open(\"classes_CR3.pkl\",\"rb\"))\n",
    "\n",
    "model = load_model(\"Chatbot_CR3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8764f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentence(sentence):\n",
    "    sen_tokens = nltk.word_tokenize(sentence)\n",
    "    sen_tokens = [lemmatizer.lemmatize(w.lower()) for w in sen_tokens]\n",
    "    return sen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    word = break_sentence(sentence)\n",
    "    word_bag = [0]*len(words)\n",
    "    for wrd in word:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == wrd:\n",
    "                word_bag[i]=1\n",
    "    return np.array(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = model.predict(np.array([bow]))[0]\n",
    "    \n",
    "    #error_threshold = 0.25\n",
    "    result = [[i,r] for i,r in enumerate(res)]# if r > error_threshold]\n",
    "    #print(result)\n",
    "    result.sort(key = lambda x:x[1], reverse = True)\n",
    "    #print(result)\n",
    "    #result_categories = []\n",
    "    #for r in result:\n",
    "        #result_categories.append({'intents':classes[r[0]], 'probability': r[1]})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61374894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(probable_intents, intents_json):\n",
    "    tag = classes[probable_intents[0][0]]\n",
    "    probability = probable_intents[0][1]\n",
    "    print(probability)\n",
    "    print(tag)\n",
    "    total_intents = intents_json['intents']\n",
    "    results = [\"I didn't get that.\", \"Please check the spellings or try asking something else.\",\"I am sorry, I didn't quite get that.\",\"Please ask again, I'll try to answer better this time.\",\"This is outside my Knowledge Please try asking something else :(\"]\n",
    "    if (probability < 0.90): #((tag == \"yes\") and (probability in arange(0.50,0.95,0.01))) or \n",
    "        ##result = \"I didn't get that, please check the spellings or try asking something else.\"\n",
    "        res = random.choice(results)\n",
    "        return res\n",
    "    else:\n",
    "        for i in total_intents:\n",
    "            if i['tag'] == tag:\n",
    "                result = random.choice(i['responses'])\n",
    "                break\n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    sentiment = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa')\n",
    "    emotion_label  = sentiment(text)\n",
    "    emotion = emotion_label[0]['label']\n",
    "    print(emotion)\n",
    "    return emotion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def song_recommendation(emotion):\n",
    "    #emotion = input(\"Enter your feeling \")\n",
    "    url = \"http://ws.audioscrobbler.com/2.0/?method=tag.gettoptracks&tag={0}&api_key=19e1d5326b6fe057e0e031b6c1131136&format=json&limit=10\".format(emotion)\n",
    "    response = requests.get(url)\n",
    "    song_data = response.json()\n",
    "    song_dict = {}\n",
    "    for i in range(5):\n",
    "        r = song_data['tracks']['track'][i]\n",
    "        song_dict[r['name']] = r['url']\n",
    "    print(\"Song recommendations:\")\n",
    "    for n,u in song_dict.items():\n",
    "        print(\"Song name:\", n)\n",
    "        print(\"Song url:\", u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chatbot is ready!! Start chatting and (Type 'quit' to stop chatting and get the song recommendations). Please refrain from using short forms and try writing full sentences. \\nEg: great  (wrong) \\n  : I'm great  (correct)    \")\n",
    "print(\"Chatbot: Hey, there!\")\n",
    "text =\"\"\n",
    "while True:\n",
    "    msg = input(\"User: \")\n",
    "    if msg.lower() == \"quit\":\n",
    "        break\n",
    "    ints = predict_class(msg)\n",
    "    res = get_response(ints, intents)\n",
    "    print(\"Chatbot:\",res)\n",
    "    text = text+\" \"+msg\n",
    "print(text)\n",
    "senti = sentiment_analysis(text)\n",
    "song_recommendation(senti)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29684636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
